## Optimizers

### ðŸ”¸ Gradient Descent

Also known as **Batch Gradient Descent**.

- Gradient Descent uses only the current gradient (point gradient).

**Update Rule:**

$$
w_{t+1} = w_t - \alpha \cdot \frac{dL}{dw_t}
$$

Where:  

$w_t$ â†’ weight at time step $t$  
$\alpha$ â†’ learning rate  
$\frac{dL}{dw_t}$ â†’ gradient of the loss function w.r.t. $w_t$  
$w_{t+1}$ â†’ updated weight for the next iteration

#### Major problem of Gradient Descent approach - 
Canâ€™t reach global minima

- Getting stuck in local minima -

![Optimizer Image 1](images/minima.jpg)

![Optimizer Image 2](images/minima2.jpg)

- Saddle points : Minima in one direction but maxima in another direction.

![Optimizer Image 3](images/saddle.jpg)

Different **Optimizers** help to reach the global minima of loss curve with higher probability.

### ðŸ”¸ Momentum based Gradient Descent

Momentum based Gradient Descent uses both historical and current gradient info.

**Update Rule:**

$$
w_{t+1} = w_t - m_t
$$

$$
m_t = \gamma \cdot m_{t-1} + (1 - \gamma) \cdot \frac{dL}{dw_t}
$$

Where:

$w_{t+1}$ â†’ updated weight for the next iteration
$w_t$  â†’ weight at time step $t$  
$m_t$ â†’ momentum at step $t$  
$\gamma$ â†’ momentum coefficient (e.g., 0.9)  
$m_{t-1}$ â†’ momentum from the previous step  
$\frac{dL}{dw_t}$ â†’ gradient of the loss function  respect to $w_t$  


$\gamma$ here is a hyperparameter.

$m_t$ is an accumulator variable.

<p>
  <img src="images/gd_vs_mt.gif" alt="Optimizer Animation" width="400"/>
</p>

> ðŸ‘‰ Click the image to view the animation.

Orange ball â†’ Momentum based GD  
Blue ball â†’ Vanilla GD


Note that momentum based GD takes more time or more oscillations to settle down than vanilla GD because of the â€œhistoryâ€ it carries.

### ðŸ”¸ Adagrad (Adaptive Gradient)

Letâ€™s consider the below 2 scenarios -

[ Scenario 1 ]

![Optimizer Image 4](images/movie.jpg)

Ex: Akshay Kumar creates movies more frequently than Aamir Khan, however the movies created by Aamir Khan have a very high impact on the IMDB ratings.

![Optimizer Image 5](images/neurons.jpg)

Weights associated  feature $x_3$ will not have equal opportunity to get trained as much as $x_2$.
Hence, sparse features wonâ€™t get sufficient opportunity to get trained.

[ Scenario 2 ]

Weights are randomly initialized due to which some weights may be closer to the minima while others may be too far. Training them for the same number of epochs would not be the right way.

**Update Rule:**

$$
w_{t+1} = w_t - \frac{\alpha}{\sqrt{v_t + \epsilon}} \cdot \frac{dL}{dw_t}
$$

$$
v_t = v_{t-1} + \left( \frac{dL}{dw_t} \right)^2
$$

Where:

$w_{t+1}$ â†’ updated weight for the next iteration  
$w_t$ â†’ weight at time step $t$  
$\alpha$ â†’ initial learning rate  
$v_t$ â†’ sum of squares of past gradients up to time $t$  
$v_{t-1}$ â†’ sum of squares of past gradients up to time $t - 1$  
$\epsilon$ â†’ small constant to prevent division by zero (e.g., $10^{-8}$)  
$\frac{dL}{dw_t}$ â†’ gradient of the loss function with respect to $w_t$

**Notes:**

- This accumulates the squared gradients over time.  
- Used in the denominator â†’ adapts the learning rate per parameter.  
- If a parameter has large past gradients, $v_t$ becomes large â†’ its learning rate becomes small, and vice versa.  

âš ï¸ **Downside of Adagrad:** The learning rate keeps shrinking over time â†’ may stop learning too early.

### ðŸ”¸ RMSprop (Root Mean Squared Propagation)

- Fixes AdaGradâ€™s problem of rapidly decreasing learning rates.

**Update Rule:**

$$
w_{t+1} = w_t - \frac{\alpha}{\sqrt{v_t + \epsilon}} \cdot \frac{dL}{dw_t}
$$

$$
v_t = \beta \cdot v_{t-1} + (1 - \beta) \cdot \left( \frac{dL}{dw_t} \right)^2
$$

Where:

$w_{t+1}$ â†’ updated weight for the next iteration  
$w_t$ â†’ weight at time step $t$  
$\alpha$ â†’ learning rate  
$v_t$ â†’ running average of past squared gradients   
$v_{t-1}$ â†’ moving average from previous step  
$\beta$ â†’ decay rate (e.g., 0.9)  
$\epsilon$ â†’ small constant to avoid division by zero (e.g., $10^{-8}$)  
$\frac{dL}{dw_t}$ â†’ gradient of the loss function with respect to $w_t$ 

- It is like AdaGrad but instead of **accumulating forever**, it **decays** older gradients --> prevents learning rate from shrinking to zero.

### ðŸ”¸ Adam (Adaptive Moment Estimation)

- Adam = RMSprop + Momentum
- Most widely used in deep learning.

**Update Rule:**

$$
w_{t+1} = w_t - \frac{\alpha}{\sqrt{\{v}_t} + \epsilon} \cdot \{m}_t
$$

$$
m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot \frac{dL}{dw_t}
$$

$$
v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot \left( \frac{dL}{dw_t} \right)^2
$$


Where:

$w_{t+1}$ â†’ updated weight for the next iteration  
$w_t$ â†’ weight at time step $t$  
$\alpha$ â†’ learning rate  
$m_t$ â†’ momentum at step $t$  
$v_t$ â†’ running average of past squared gradients  
$\beta_1$ â†’ decay rate for the momentum (e.g., 0.9)  
$\beta_2$ â†’ decay rate for the velocity (e.g., 0.999)  
$\epsilon$ â†’ small constant to prevent division by zero (e.g., $10^{-8}$)  
$\frac{dL}{dw_t}$ â†’ gradient of the loss function with respect to $w_t$  

---
### Visual comparison of Optimizers

<p>
  <img src="images/optimizer_compare.gif" alt="Optimizers Comparision" width="400"/>
</p>

> ðŸ‘‰ Click the image to view the animation.

> Animation of 5 gradient descent methods on a surface: gradient descent (cyan), momentum (magenta), AdaGrad (white), RMSProp (green), Adam (blue). Left well is the global minimum; right well is a local minimum.

---
### Equations for all Optimizers

#### ðŸ”¸ Gradient Descent

$$
w_{t+1} = w_t - \alpha \cdot \frac{dL}{dw_t}
$$

#### ðŸ”¸ Momentum based Gradient Descent

$$
w_{t+1} = w_t - m_t
$$

$$
m_t = \gamma \cdot m_{t-1} + (1 - \gamma) \cdot \frac{dL}{dw_t}
$$

#### ðŸ”¸ Adagrad

$$
w_{t+1} = w_t - \frac{\alpha}{\sqrt{v_t + \epsilon}} \cdot \frac{dL}{dw_t}
$$

$$
v_t = v_{t-1} + \left( \frac{dL}{dw_t} \right)^2
$$

#### ðŸ”¸ RMSprop

$$
w_{t+1} = w_t - \frac{\alpha}{\sqrt{v_t + \epsilon}} \cdot \frac{dL}{dw_t}
$$

$$
v_t = \beta \cdot v_{t-1} + (1 - \beta) \cdot \left( \frac{dL}{dw_t} \right)^2
$$

#### ðŸ”¸ Adam

$$
w_{t+1} = w_t - \frac{\alpha}{\sqrt{\{v}_t} + \epsilon} \cdot \{m}_t
$$

$$
m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot \frac{dL}{dw_t}
$$

$$
v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot \left( \frac{dL}{dw_t} \right)^2
$$
